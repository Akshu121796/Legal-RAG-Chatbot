# ğŸ§  Local Legal RAG Chatbot (Chainlit + LLaMA + FAISS)

A local Retrieval-Augmented Generation (RAG) chatbot that allows users to upload a PDF and ask questions about its content â€” all without any OpenAI or cloud dependencies.

## âœ… Features
- ğŸ” Context-aware answers from uploaded PDF
- ğŸ“ Drag & drop PDF support
- ğŸ§  Local LLM via `llama-cpp-python`
- ğŸ“¦ Uses HuggingFace sentence-transformers for embedding
- ğŸš« No internet required once set up

---

### ğŸ“Project structure
legal-rag-chatbot/
â”‚
â”œâ”€â”€ app.py                      
â”œâ”€â”€ requirements.txt            
â”œâ”€â”€ README.md                   
â”œâ”€â”€ .gitignore                  
â”œâ”€â”€ models/                      
â”‚   â””â”€â”€ mistral-7b-instruct-v0.1.Q4_K_M.gguf
â”œâ”€â”€ db/                        
â”‚   â””â”€â”€ index/                   
â”œâ”€â”€ temp/                        
â””â”€â”€ assets/ (optional)        
    â””â”€â”€ demo.png

## ğŸ›  Setup Instructions

###
```bash
git clone https://github.com/Akshu121796/legal-rag-chatbot.git
cd legal-rag-chatbot
```

### 2. Download the LLM MOdel
```bash
Visit: mistral-7b-instruct-v0.1 GGUF (Q4_K_M)
Download:" mistral-7b-instruct-v0.1.Q4_K_M.gguf"(place in models/)
```

### 3. Create and activate virtual environment (optional)
```bash
python -m venv venv
source venv/bin/activate  ( on Windows use venv\Scripts\activate)
```

### 4. Install dependencies
```bash
pip install -r requirements.txt
```

### 5. Run the Chatbot Locally
```bash
chainlit run app.py 
```


### ğŸ“ Note
- If the model is too large for your system, use a smaller quantized .gguf variant.

- Use only legal and public PDFs for testing.

## ğŸ¤ Contributing
Feel free to fork the repo and submit pull requests.

